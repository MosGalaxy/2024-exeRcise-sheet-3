---
title: "Exercise #3"
subtitle: "Fortgeschrittene Statistische Software f√ºr NF"
author: "Shuman Mo (12468087)"
date: "`r Sys.Date()`"
output: distill::distill_article
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, message=FALSE}
library(tidyverse)
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
install.packages("janitor")
library(janitor)
library(ggplot2)
```

## Exercise 1: Initializing git

For this whole exercise sheet we will be tracking all our changes to it
in git.

### a) Start by initializing a new R project with git support, called

`2024-exeRcise-sheet-3`. If you forgot how to do this, you can follow
this
[guide](https://malikaihle.github.io/Introduction-RStudio-Git-GitHub/rstudio_project.html).

### b) Commit the files generated by Rstudio.

### c) For all of the following tasks in this exercise sheet we ask you to

always commit your changes after finishing each sub-task e.g. create a
commit after task *1d*, *1e* etc.

### d) Name 2 strengths and 2 weaknesses of git (Don't forget to create a

```         
commit after this answer, see *1c*).
```

-   Strength

**Distributed Version Control**: Git's distributed architecture allows
every user to have a complete copy of the repository, including its full
history. This ensures that work can be done offline and reduces the risk
of data loss since every user has a full backup.

**Branching and Merging**: Git provides powerful branching and merging
capabilities. Branches in Git are lightweight and can be created
quickly, allowing users to experiment and develop features in isolation
from the main codebase.

-   Weaknesses

**Complexity**: Git has a steep learning curve, especially for
beginners. Its command-line interface and the complexity of certain
commands can be daunting.

**Performance with Large Repositories**: While Git handles small to
medium-sized repositories efficiently, it can experience performance
issues with very large repositories or repositories with a large number
of files and long history.

### e) Knit this exercise sheet. Some new files will automatically be generated when knitting the sheet e.g. the HTML page. Ignore these files, as we only want to track the source files themselves.

## Exercise 2: Putting your Repository on GitHub

For this task you will upload your solution to GitHub.

### a) Create a new repository on GitHub in your account named exeRcise-sheet-3. Make sure you create a **public repository** so we are able to see it for grading. Add the link to the repository below:

https://github.com/MosGalaxy/2024-exeRcise-sheet-3.git

### b) Push your code to this new repository by copying and executing the snippet on github listed under or push an existing repository from the command line.

### c) Regularly push your latest changes to GitHub again and especially do so when you are finished with this sheet.

## Exercise 3: Baby-Names in Munich

Download the latest open datasets on given names ("Vornamen") from the
open data repository of the city of Munich for the years `2023` and
`2022`.

Link: <https://opendata.muenchen.de/dataset/vornamen-von-neugeborenen>

### a) Download the data for both years and track it in git. For small

```         
datasets like these adding them to git is not a problem.
```


### b) Load the data for both years into R. Check the type of the count variable ("Anzahl") and look into the data to determine why it is
not numeric? Fix the problem in an appropriate manner, it is OK if
some of the counts are inaccurate because of this. Explain your
solution and the repercussions.

```{r}
# Load data for 2023 and 2022
vornamen_2023 <- read_csv("DATA/vornamen-muenchen-2023.csv")
vornamen_2022 <- read_csv("DATA/open_data_portal_2022.csv")

# Check the type of the "Anzahl" variable for both years
class(vornamen_2023$Anzahl)
class(vornamen_2022$Anzahl)
```

After examining the data, we found that some counts are represented as non-numeric values (e.g., "4 oder weniger"). We can use ifelse to conditionally convert "4 oder weniger" to 4 while keeping other values as numeric. 

Compared to converting to NA, this approach retains the sample size and contextual information associated with the "<= 4" values if necessary.

```{r}
# Convert non-numeric values to 4 and the column to numeric
vornamen_2023$Anzahl <- ifelse(vornamen_2023$Anzahl == "4 oder weniger", 4, as.numeric(vornamen_2023$Anzahl))

vornamen_2022$Anzahl <- ifelse(vornamen_2022$Anzahl == "4 oder weniger", 4, as.numeric(vornamen_2022$Anzahl))
```

### c) Calculate the total number of babies born in Munich in 2022 and 2023. Which year had the bigger baby-boom?

```{r}
# Calculate the total number of babies born in Munich in 2023 and 2022
total_babies_2023 <- sum(vornamen_2023$Anzahl)
total_babies_2022 <- sum(vornamen_2022$Anzahl)
```

The baby-boom was bigger in 2022.

### d) Add a new column `year` to both datasets which holds the correct year for each.

```{r}
# Add a new column 'year' to both datasets
vornamen_2022$year <- 2023
vornamen_2023$year <- 2022

# Print the datasets to verify the changes
head(vornamen_2023)
head(vornamen_2022)
```

### e) Combine both datasets into one using `bind_rows()`.

```{r}
# Combine both datasets into one
combined_data <- bind_rows(vornamen_2023, vornamen_2022)

# Print the combined dataset
head(combined_data)
```

### f) Combine the counts for same names to determine the most popular names across both years. Print out the top 10 names in a nicely
formatted table for both years. Include a table caption.

```{r}         
# Group the data by name and sum the counts
popular_names <- combined_data %>%
  group_by(Vorname) %>%
  summarise(Total_Count = sum(Anzahl)) %>%
  arrange(desc(Total_Count))

# Print out the top 10 names for both years in a nicely formatted table
cat("\nTable: Top 10 Most Popular Names Across 2022 and 2023\n")
print(popular_names[1:10, ], row.names = FALSE)
```

## Exercise 4: Open Analysis

This exercise is a bit more open-ended. You can choose any dataset from
[Our World in Data](https://ourworldindata.org/) and analyze it, while
determining the research question yourself.

### a) Go to <https://github.com/owid/owid-datasets/tree/master/datasets> and choose a dataset that interests you. You can have a look at <https://ourworldindata.org/> to gather some inspiration.

### b) Download the dataset and track it in git.

### c) Put the name / title of the dataset and a link to it below.

-   Dataset Name: Indicators for "What is PPP?" - World Bank
-   Link: <https://github.com/owid/owid-datasets/tree/master/datasets/Indicators%20for%20!What%20is%20PPP!%20%20-%20World%20Bank#indicators-for-what-is-ppp----world-bank>

### d) Come up with a (research) question you want to answer with the data and briefly explain why you believe this is an interesting question within one sentence. It should be a question that can be answered with the dataset and using R.

**Research Question**: How does the Price Level Ratio (PLR) affect the discrepancy between GDP per capita (PPP) and GDP per capita (USD) across countries?

**Explaination**: This question explores the impact of cost of living on the perceived economic output per individual, providing insights into economic well-being across different countries.

### e) Use R to answer your chosen question.

```{r}
# Load the dataset in R
PLR_GDPcp_1990_2015 <- read_csv("DATA/PLR_GDPcp_1990_2015.csv")

# View the data structure
glimpse(PLR_GDPcp_1990_2015)
head(PLR_GDPcp_1990_2015)
str(PLR_GDPcp_1990_2015)
```


```{r}
# Standardize the column names to snake_case
PLR_GDPcp_1990_2015 <- PLR_GDPcp_1990_2015 %>%
  clean_names() 

# Abbreviate the column names
PLR_GDPcp_1990_2015 <- PLR_GDPcp_1990_2015 %>%
  rename(
    plr = price_level_ratio_of_ppp_conversion_factor_gdp_to_market_exchange_rate_world_bank,
    gdp_pc_ppp = gdp_per_capita_ppp_int_adjusted_world_bank,
    gdp_pc_usd = gdp_per_capita_us_market_exchange_world_bank
  ) 
```

```{r}
# Check for missing values
summary(PLR_GDPcp_1990_2015)

# Remove rows with missing values
PLR_GDPcp_2015 <- na.omit(PLR_GDPcp_1990_2015)
```

```{r}
# Filter out rows where price level ratio is greater than 1
higher_lc_2015 <- PLR_GDPcp_2015 %>% 
  filter(plr > 1)

print(higher_lc_2015) # Countries of higher living costs in 2015
```

The table indicates that goods and services were relatively more expensive in these countries compared to the United States back in 2015. 

However, this does not directly imply a higher inflation rate of the year. To examine if there was a higher inflation rate, we need to take a closer look at the differences between GDP per capita (PPP) and GDP per capita (USD).

```{r}
# Compute the difference between GDP per capita (PPP) and GDP per capita (USD)
PLR_GDPcp_2015 <- PLR_GDPcp_2015 %>%
  mutate(gdp_diff = gdp_pc_ppp - gdp_pc_usd)

high_inflated_2015 <- PLR_GDPcp_2015 %>%
  filter(gdp_diff < 0)

print(high_inflated_2015)
```

The table shows these countries experienced a high inflation back in 2015.

```{r}
# Calculate correlation
correlation <- cor(higher_lc_2015$plr, high_inflated_2015$gdp_diff)

# Print correlation
print(correlation)
```
The correlation between PLR and the GDP disparity is -0.9859214, indicating a strong positive correlation. In this case, as PLR increases, the GDP disparity tends to widen.

### f) Create a meaningful plot / figure with the dataset. Make sure to provide a figure caption (via the chunk options / Rmarkdown) and correctly label the figure.

```{r}
# Create a scatter plots
ggplot(PPP_GDP_2015, aes(x = pl_ratio, y = gdp_cap_ppp)) +
  geom_point() +
  labs(x = "Price Level Ratio", y = "GDP per Capita (PPP)") +
  ggtitle("Relationship between Price Level Ratio and GDP per Capita (PPP)")

ggplot(PPP_GDP_2015, aes(x = pl_ratio, y = gdp_cap_usd)) +
  geom_point() +
  labs(x = "Price Level Ratio", y = "GDP per Capita (USD)") +
  ggtitle("Relationship between Price Level Ratio and GDP per Capita (USD)")

```
## Final Note

Make sure to push all your commits and changes to GitHub before
submittining the exercise sheet.
